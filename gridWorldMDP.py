possible_initial_states_for_grid_world = [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4),
                                          (1, 0), (1, 1), (1, 2), (1, 3), (1, 4),
                                          (2, 0), (2, 1), (2, 3), (2, 4),
                                          (3, 0), (3, 1), (3, 3), (3, 4),
                                          (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]

terminal_states_for_grid_world = [(4, 4)]

obstacle_states_for_grid_world = [(2, 2), (3, 2)]

optimal_policy_for_grid_world = {(0, 0): 'AR', (0, 1): 'AR', (0, 2): 'AR', (0, 3): 'AD', (0, 4): 'AD',
                                 (1, 0): 'AR', (1, 1): 'AR', (1, 2): 'AR', (1, 3): 'AD', (1, 4): 'AD',
                                 (2, 0): 'AU', (2, 1): 'AU', (2, 3): 'AD', (2, 4): 'AD',
                                 (3, 0): 'AU', (3, 1): 'AU', (3, 3): 'AD', (3, 4): 'AD',
                                 (4, 0): 'AU', (4, 1): 'AU', (4, 2): 'AR', (4, 3): 'AR'}

m_for_grid_world, n_for_grid_world = 5, 5

rewards_for_grid_world = {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, (0, 4): 0,
                          (1, 0): 0, (1, 1): 0, (1, 2): 0, (1, 3): 0, (1, 4): 0,
                          (2, 0): 0, (2, 1): 0, (2, 3): 0, (2, 4): 0,
                          (3, 0): 0, (3, 1): 0, (3, 3): 0, (3, 4): 0,
                          (4, 0): 0, (4, 1): 0, (4, 2): -10, (4, 3): 0, (4, 4): 10}

gamma_for_grid_world = 0.9

optimal_policy_v_for_grid_world = [[4.0187, 4.5548, 5.1575, 5.8336, 6.4553],
                                   [4.3716, 5.0324, 5.8013, 6.6473, 7.3907],
                                   [3.8672, 4.3900, 0.0000, 7.5769, 8.4637],
                                   [3.4182, 3.8319, 0.0000, 8.5738, 9.6946],
                                   [2.9977, 2.9309, 6.0733, 9.6946, 0.0000]]
